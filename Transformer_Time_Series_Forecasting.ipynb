{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "âœ… **Advanced Time Series Forecasting with Attention-Based Transformers**\n",
        "\n",
        "This project focuses on implementing and evaluating deep learning models for multivariate time series forecasting, including:\n",
        "\n",
        "\n",
        "*   Transformer with Self-Attention (Advanced Model)\n",
        "*   LSTM (Baseline Model)\n",
        "\n",
        "ðŸ“Œ **Project Details**\n",
        "\n",
        "\n",
        "*   Dataset Type: Synthetic Multivariate Time Series\n",
        "*   Dataset Size: 6500+ observations\n",
        "\n",
        "\n",
        "*   Frameworks Used: PyTorch, NumPy, Scikit-learn\n",
        "*   Evaluation Metrics: MAE (Mean Absolute Error), RMSE (Root Mean Squared Error)\n",
        "\n",
        "\n",
        "This work demonstrates the use of positional encoding and self-attention mechanisms for multi-step forecasting and compares results against a traditional recurrent baseline model."
      ],
      "metadata": {
        "id": "T1-AmU3XqIIQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CCfcY-gOi6os",
        "outputId": "c745db29-1696-4b54-e148-c953fc148cab"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.9.0+cpu)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (2.0.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.6.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (3.10.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.20.3)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.3)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.16.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.5.3)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (4.61.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (3.3.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n"
          ]
        }
      ],
      "source": [
        "# Install required libraries\n",
        "!pip install torch numpy pandas scikit-learn matplotlib\n",
        "\n",
        "# Core imports\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
        "\n",
        "import matplotlib.pyplot as plt\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Device:\", device)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L2Q16IKskPPp",
        "outputId": "45ca475a-4608-4151-ca1a-2c3565a7ecc9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Multivariate synthetic dataset with interactions\n",
        "def generate_data(n_steps=6500):\n",
        "    t = np.arange(n_steps)\n",
        "\n",
        "    s1 = 0.03*t + np.sin(0.02*t) + np.random.normal(0, 0.2, n_steps)\n",
        "    s2 = np.cos(0.015*t) + 0.6*s1 + np.random.normal(0, 0.25, n_steps)\n",
        "    s3 = np.sin(0.01*t) + np.cos(0.02*t) + np.random.normal(0, 0.3, n_steps)\n",
        "\n",
        "    return np.stack([s1, s2, s3], axis=1)\n",
        "\n",
        "data = generate_data()\n",
        "print(\"Dataset shape:\", data.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QGa1gtx0rbBx",
        "outputId": "6b236689-967a-4124-e6b6-4e6888a4da43"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset shape: (6500, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "scaler = MinMaxScaler()\n",
        "data_scaled = scaler.fit_transform(data)"
      ],
      "metadata": {
        "id": "WZj6HjB4tMqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_sequences(data, input_len, output_len):\n",
        "    X, y = [], []\n",
        "    for i in range(len(data) - input_len - output_len):\n",
        "        X.append(data[i:i+input_len])\n",
        "        y.append(data[i+input_len:i+input_len+output_len])\n",
        "    return np.array(X), np.array(y)\n",
        "\n",
        "INPUT_LEN = 48     # Tuned\n",
        "OUTPUT_LEN = 12    # Multi-step forecast\n",
        "\n",
        "X, y = create_sequences(data_scaled, INPUT_LEN, OUTPUT_LEN)\n",
        "print(X.shape, y.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DVxJAtn_tUS2",
        "outputId": "6f882161-9216-4433-e561-eaa1da9ee6f6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(6440, 48, 3) (6440, 12, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "split = int(0.8 * len(X))\n",
        "X_train, X_test = X[:split], X[split:]\n",
        "y_train, y_test = y[:split], y[split:]\n"
      ],
      "metadata": {
        "id": "ivo8QHzzvtNg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TimeSeriesDataset(Dataset):\n",
        "    def __init__(self, X, y):\n",
        "        self.X = torch.tensor(X, dtype=torch.float32)\n",
        "        self.y = torch.tensor(y, dtype=torch.float32)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.X[idx], self.y[idx]\n",
        "\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    TimeSeriesDataset(X_train, y_train),\n",
        "    batch_size=32,\n",
        "    shuffle=True\n",
        ")\n",
        "\n",
        "test_loader = DataLoader(\n",
        "    TimeSeriesDataset(X_test, y_test),\n",
        "    batch_size=32\n",
        ")\n"
      ],
      "metadata": {
        "id": "7miixYgNvxhb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, max_len=500):\n",
        "        super().__init__()\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        pos = torch.arange(0, max_len).unsqueeze(1)\n",
        "        div = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
        "\n",
        "        pe[:, 0::2] = torch.sin(pos * div)\n",
        "        pe[:, 1::2] = torch.cos(pos * div)\n",
        "        self.pe = pe.unsqueeze(0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x + self.pe[:, :x.size(1)].to(x.device)\n"
      ],
      "metadata": {
        "id": "0erXFfdkxGS1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerForecast(nn.Module):\n",
        "    def __init__(self, input_dim, d_model=64, nhead=4, num_layers=2):\n",
        "        super().__init__()\n",
        "\n",
        "        self.embedding = nn.Linear(input_dim, d_model)\n",
        "        self.pos_encoding = PositionalEncoding(d_model)\n",
        "\n",
        "        encoder_layer = nn.TransformerEncoderLayer(\n",
        "            d_model=d_model,\n",
        "            nhead=nhead,\n",
        "            batch_first=True\n",
        "        )\n",
        "        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
        "\n",
        "        self.output_layer = nn.Linear(d_model, input_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x)\n",
        "        x = self.pos_encoding(x)\n",
        "        x = self.encoder(x)\n",
        "        return self.output_layer(x[:, -OUTPUT_LEN:, :])\n"
      ],
      "metadata": {
        "id": "1g6WXVWExJls"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LSTMModel(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim=64):\n",
        "        super().__init__()\n",
        "        self.lstm = nn.LSTM(input_dim, hidden_dim, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_dim, input_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out, _ = self.lstm(x)\n",
        "        return self.fc(out[:, -OUTPUT_LEN:, :])\n"
      ],
      "metadata": {
        "id": "VHsS7nvRxOcu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model, epochs, lr):\n",
        "    model.to(device)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "    loss_fn = nn.MSELoss()\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "\n",
        "        for xb, yb in train_loader:\n",
        "            xb, yb = xb.to(device), yb.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            preds = model(xb)\n",
        "            loss = loss_fn(preds, yb)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{epochs} | Loss: {total_loss/len(train_loader):.5f}\")\n"
      ],
      "metadata": {
        "id": "KVCqQzEpxRNt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "configs = [\n",
        "    {\"epochs\": 10, \"lr\": 0.001},\n",
        "    {\"epochs\": 20, \"lr\": 0.001}\n",
        "]\n",
        "\n",
        "best_model = None\n",
        "best_rmse = float(\"inf\")\n",
        "\n",
        "for cfg in configs:\n",
        "    print(\"\\nTraining Transformer:\", cfg)\n",
        "    model = TransformerForecast(input_dim=3)\n",
        "    train_model(model, cfg[\"epochs\"], cfg[\"lr\"])\n",
        "\n",
        "    mae, rmse = 0, 0\n",
        "    model.eval()\n",
        "    preds, actuals = [], []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for xb, yb in test_loader:\n",
        "            xb = xb.to(device)\n",
        "            preds.append(model(xb).cpu().numpy())\n",
        "            actuals.append(yb.numpy())\n",
        "\n",
        "    preds = np.concatenate(preds)\n",
        "    actuals = np.concatenate(actuals)\n",
        "\n",
        "    rmse = np.sqrt(mean_squared_error(actuals.flatten(), preds.flatten()))\n",
        "    print(\"RMSE:\", rmse)\n",
        "\n",
        "    if rmse < best_rmse:\n",
        "        best_rmse = rmse\n",
        "        best_model = model\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2DDw0GOExUtK",
        "outputId": "27cb9801-a2de-43ac-816c-f000b1d6d596"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training Transformer: {'epochs': 10, 'lr': 0.001}\n",
            "Epoch 1/10 | Loss: 0.04985\n",
            "Epoch 2/10 | Loss: 0.00410\n",
            "Epoch 3/10 | Loss: 0.00276\n",
            "Epoch 4/10 | Loss: 0.00242\n",
            "Epoch 5/10 | Loss: 0.00226\n",
            "Epoch 6/10 | Loss: 0.00218\n",
            "Epoch 7/10 | Loss: 0.00212\n",
            "Epoch 8/10 | Loss: 0.00203\n",
            "Epoch 9/10 | Loss: 0.00196\n",
            "Epoch 10/10 | Loss: 0.00195\n",
            "RMSE: 0.04429778892425369\n",
            "\n",
            "Training Transformer: {'epochs': 20, 'lr': 0.001}\n",
            "Epoch 1/20 | Loss: 0.05019\n",
            "Epoch 2/20 | Loss: 0.00468\n",
            "Epoch 3/20 | Loss: 0.00292\n",
            "Epoch 4/20 | Loss: 0.00260\n",
            "Epoch 5/20 | Loss: 0.00236\n",
            "Epoch 6/20 | Loss: 0.00221\n",
            "Epoch 7/20 | Loss: 0.00211\n",
            "Epoch 8/20 | Loss: 0.00210\n",
            "Epoch 9/20 | Loss: 0.00198\n",
            "Epoch 10/20 | Loss: 0.00198\n",
            "Epoch 11/20 | Loss: 0.00191\n",
            "Epoch 12/20 | Loss: 0.00192\n",
            "Epoch 13/20 | Loss: 0.00188\n",
            "Epoch 14/20 | Loss: 0.00181\n",
            "Epoch 15/20 | Loss: 0.00181\n",
            "Epoch 16/20 | Loss: 0.00177\n",
            "Epoch 17/20 | Loss: 0.00174\n",
            "Epoch 18/20 | Loss: 0.00174\n",
            "Epoch 19/20 | Loss: 0.00174\n",
            "Epoch 20/20 | Loss: 0.00168\n",
            "RMSE: 0.05670946352903893\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lstm = LSTMModel(input_dim=3)\n",
        "train_model(lstm, epochs=20, lr=0.001)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O37HXSRy26UU",
        "outputId": "5228a711-f384-4569-f610-d9a9bcbcee4d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20 | Loss: 0.04938\n",
            "Epoch 2/20 | Loss: 0.00298\n",
            "Epoch 3/20 | Loss: 0.00249\n",
            "Epoch 4/20 | Loss: 0.00233\n",
            "Epoch 5/20 | Loss: 0.00222\n",
            "Epoch 6/20 | Loss: 0.00208\n",
            "Epoch 7/20 | Loss: 0.00183\n",
            "Epoch 8/20 | Loss: 0.00177\n",
            "Epoch 9/20 | Loss: 0.00174\n",
            "Epoch 10/20 | Loss: 0.00173\n",
            "Epoch 11/20 | Loss: 0.00173\n",
            "Epoch 12/20 | Loss: 0.00169\n",
            "Epoch 13/20 | Loss: 0.00165\n",
            "Epoch 14/20 | Loss: 0.00165\n",
            "Epoch 15/20 | Loss: 0.00162\n",
            "Epoch 16/20 | Loss: 0.00164\n",
            "Epoch 17/20 | Loss: 0.00164\n",
            "Epoch 18/20 | Loss: 0.00159\n",
            "Epoch 19/20 | Loss: 0.00159\n",
            "Epoch 20/20 | Loss: 0.00157\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(model):\n",
        "    model.eval()\n",
        "    preds, actuals = [], []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for xb, yb in test_loader:\n",
        "            xb = xb.to(device)\n",
        "            preds.append(model(xb).cpu().numpy())\n",
        "            actuals.append(yb.numpy())\n",
        "\n",
        "    preds = np.concatenate(preds)\n",
        "    actuals = np.concatenate(actuals)\n",
        "\n",
        "    mae = mean_absolute_error(actuals.flatten(), preds.flatten())\n",
        "    rmse = np.sqrt(mean_squared_error(actuals.flatten(), preds.flatten()))\n",
        "    return mae, rmse\n"
      ],
      "metadata": {
        "id": "tsp1uvjD3C3G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mae_t, rmse_t = evaluate(best_model)\n",
        "mae_l, rmse_l = evaluate(lstm)\n",
        "\n",
        "print(\"FINAL RESULTS\")\n",
        "print(\"-----------------------------\")\n",
        "print(f\"Transformer â†’ MAE: {mae_t:.4f}, RMSE: {rmse_t:.4f}\")\n",
        "print(f\"LSTM        â†’ MAE: {mae_l:.4f}, RMSE: {rmse_l:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3zZmNSoH3IGk",
        "outputId": "6fb477a8-6fb3-4e01-c0f8-33f355464cbe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FINAL RESULTS\n",
            "-----------------------------\n",
            "Transformer â†’ MAE: 0.0335, RMSE: 0.0443\n",
            "LSTM        â†’ MAE: 0.0273, RMSE: 0.0422\n"
          ]
        }
      ]
    }
  ]
}